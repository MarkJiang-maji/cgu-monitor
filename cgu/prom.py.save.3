ode', ['node'])
gpu_allocatable_metric = Gauge('node_gpu_allocatable', 'Allocatable GPUs per node', ['node'])

node_usage_cpu_metric = Gauge('node_usage_cpu', 'CPU usage per node', ['node'])
node_usage_gpu_metric = Gauge('node_usage_gpu', 'GPU usage per node', ['node'])
namespace_usage_cpu_metric = Gauge('namespace_usage_cpu', 'CPU usage per namespace', ['namespace'])
namespace_usage_gpu_metric = Gauge('namespace_usage_gpu', 'GPU usage per namespace', ['namespace'])
notebook_usage_cpu = Gauge('notebook_cpu_usage', 'CPU usage of notebooks', ['node', 'namespace', 'notebook'])
notebook_usage_memory = Gauge('notebook_memory_usage', 'Memory usage of notebooks', ['node', 'namespace', 'notebook'])
notebook_usage_gpu = Gauge('notebook_gpu_usage', 'GPU usage of notebooks', ['node', 'namespace', 'notebook'])


node_usage_memory_metric = Gauge('node_memory_usage', 'Memory usage of nodes', ['node'])  # Fixed here
memory_allocatable_metric = Gauge('node_memory_allocatable', 'Allocatable memory per node', ['node'])

def get_cpu_used(node_name, namespace):
    pods = v1.list_namespaced_pod(namespace).items
    cpu_used = 0

    for pod in pods:
        if pod.spec.node_name == node_name:
            for container in pod.spec.containers:
                resources = container.resources
                if resources and resources.requests and 'cpu' in resources.requests:
                    cpu_request = resources.requests['cpu']
                    if cpu_request[-1] == 'm':
                        cpu_used += int(cpu_request[:-1])/1000.0
                    else:
                        cpu_used += int(cpu_request)
    return cpu_used

def get_gpu_used(node_name, namespace):
    pods = v1.list_namespaced_pod(namespace).items
    gpu_used = 0

    for pod in pods:
        if pod.spec.node_name == node_name:
            for container in pod.spec.containers:
                resources = container.resources
                if resources and resources.requests:
                    for resource_name, resource_quantity in resources.requests.items():
                        if 'gpu' in resource_name.lower():
                            gpu_used += int(resource_quantity)
    return gpu_used

def get_gpu_allocatable(pod):
    for container in pod.spec.containers:
        resources = container.resources
        if resources and resources.limits:
            for resource_name, resource_quantity in resources.limits.items():
                if 'gpu' in resource_name.lower():
                    return resource_quantity
    return '0'  # 如果没有找到GPU资源，返回0

def get_notebook_name(pod):
    if pod.metadata.name.endswith('-0'):
        return pod.metadata.name  # '-0'
    return None

def convert_memory_allocation_to_float(memory_alloc_str):
    if memory_alloc_str.endswith('Ki'):
        return float(memory_alloc_str[:-2]) / (1024 * 1024)
    elif memory_alloc_str.endswith('Mi'):
        return float(memory_alloc_str[:-2]) / 1024.0
    elif memory_alloc_str.endswith('Gi'):
        return float(memory_alloc_str[:-2])
    else:
        return float(memory_alloc_str) / (1024 * 1024 * 1024)

def get_node_memory_usage(node_name):
    try:
        metrics = metrics_v1.get_cluster_custom_object("metrics.k8s.io", "v1beta1", "nodes", node_name)
        usage_memory = metrics['usage']['memory']
        if usage_memory.endswith('Ki'):
            return float(usage_memory[:-2]) / (1024 * 1024)
        elif usage_memory.endswith('Mi'):
            return float(usage_memory[:-2]) / 1024.0
        elif usage_memory.endswith('Gi'):
            return float(usage_memory[:-2])
        else:
            return float(usage_memory) / (1024 * 1024 * 1024)
    except ApiException as e:
        print(f"Exception when calling CustomObjectsApi->get_cluster_custom_object: {e}")
        return 0

def update_metrics():
    nodes = v1.list_node().items
    namespaces = v1.list_namespace().items
    pods = v1.list_pod_for_all_namespaces().items  # 列出所有命名空间中的所有Pod
    node_usage_cpu = {}
    node_usage_gpu = {}
    namespace_usage_cpu = {}
    namespace_usage_gpu = {}
    notebook_usage_cpu_data = {}
    notebook_usage_gpu_data = {}
    node_usage_memory = {}

    for node in nodes:
        node_name = node.metadata.name
        node_usage_cpu[node_name] = 0
        node_usage_gpu[node_name] = 0
        node_usage_memory[node_name] = get_node_memory_usage(node_name)

        cpu_allocatable = node.status.allocatable.get('cpu', '0')
        gpu_allocatable = node.status.allocatable.get('nvidia.com/gpu', '0')
        cpu_allocatable_metric.labels(node=node_name).set(cpu_allocatable)
        gpu_allocatable_metric.labels(node=node_name).set(gpu_allocatable)

        memory_allocatable = node.status.allocatable.get('memory', '0')
        memory_allocatable_metric.labels(node=node_name).set(convert_memory_allocation_to_float(memory_allocatable))
        node_usage_memory_metric.labels(node=node_name).set(node_usage_memory[node_name])  # Fixed here

    for namespace in namespaces:
        namespace_name = namespace.metadata.name
        namespace_usage_cpu[namespace_name] = 0
        namespace_usage_gpu[namespace_name] = 0

    for pod in pods:
        node_name = pod.spec.node_name
        namespace_name = pod.metadata.namespace  # 获取 Pod 所在的命名空间
        notebook_name = get_notebook_name(pod)
        if notebook_name:
            if notebook_name is not None:
                cpu_allocated = get_cpu_used(node_name, namespace_name)
                gpu_allocated = get_gpu_used(node_name, namespace_name)

                namespace_usage_cpu[namespace_name] += cpu_allocated
                namespace_usage_gpu[namespace_name] += gpu_allocated

                notebook_usage_cpu.labels(node=node_name, namespace=namespace_name, notebook=notebook_name).set(cpu_allocated)
                notebook_usage_memory.labels(node=node_name, namespace=namespace_name, notebook=notebook_name).set(0)  # 未实现内存使用量
                notebook_usage_gpu.labels(node=node_name, namespace=namespace_name, notebook=notebook_name).set(gpu_allocated)

                notebook_usage_cpu_data[(node_name, namespace_name, notebook_name)] = cpu_allocated
                notebook_usage_gpu_data[(node_name, namespace_name, notebook_name)] = gpu_allocated

    return node_usage_cpu, node_usage_gpu, namespace_usage_cpu, namespace_usage_gpu, notebook_usage_cpu_data, notebook_usage_gpu_data

if __name__ == "__main__":
    start_http_server(8080)  # 确保端口未被占用

    while True:
        node_usage_cpu, node_usage_gpu, namespace_usage_cpu, namespace_usage_gpu, notebook_usage_cpu_data, notebook_usage_gpu_data = update_metrics()
        print("Node Usage CPU:", node_usage_cpu)
        print("Node Usage GPU:", node_usage_gpu)
        print("Namespace Usage CPU:", namespace_usage_cpu)
        print("Namespace Usage GPU:", namespace_usage_gpu)
        print("Notebook Usage CPU:", notebook_usage_cpu_data)
        print("Notebook Usage GPU:", notebook_usage_gpu_data)
        print("running")
        time.sleep(60)  # 每 60 秒更新一次

